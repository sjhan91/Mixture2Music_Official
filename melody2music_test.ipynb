{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import IPython.display\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from time import time\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from utils.data import *\n",
    "from utils.model import *\n",
    "from utils.metric import *\n",
    "from utils.common_utils import *\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "%load_ext blackcellmagic\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model with GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# load config file\n",
    "with open(\"./config.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data here!\n",
    "test_file_list = glob(config[\"test_folder_path\"] + \"/*\")\n",
    "print(\"The number of test: %d\" % len(test_file_list))\n",
    "\n",
    "# dataloader\n",
    "test_params = {\"batch_size\": config[\"batch_size\"], \n",
    "               \"shuffle\": False, \n",
    "               \"pin_memory\": True, \n",
    "               \"num_workers\": 4}\n",
    "\n",
    "test_set = DataLoader(DatasetSampler(test_file_list), **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransUNet\n",
    "diffusion = TransUNet_Lightning(\n",
    "    config[\"in_ch\"],\n",
    "    config[\"out_ch\"],\n",
    "    config[\"num_layers\"],\n",
    "    config[\"d_model\"],\n",
    "    config[\"latent_dim\"],\n",
    "    config[\"time_emb_dim\"],\n",
    "    config[\"time_steps\"],\n",
    "    rate=config[\"rate\"],\n",
    ")\n",
    "\n",
    "diffusion = diffusion.load_from_checkpoint(\n",
    "    config[\"tranunet_model_path\"],\n",
    "    in_ch=config[\"in_ch\"],\n",
    "    out_ch=config[\"out_ch\"],\n",
    "    num_layers=config[\"num_layers\"],\n",
    "    d_model=config[\"d_model\"],\n",
    "    latent_dim=config[\"latent_dim\"],\n",
    "    time_emb_dim=config[\"time_emb_dim\"],\n",
    "    time_steps=config[\"time_steps\"],\n",
    "    rate=config[\"rate\"],\n",
    ")\n",
    "\n",
    "diffusion = diffusion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Decoder\n",
    "decoder = SimpleDecoder_Lightning(\n",
    "    config[\"in_ch\"], config[\"out_ch\"], diffusion, config[\"latent_dim\"]\n",
    ")\n",
    "\n",
    "decoder = decoder.load_from_checkpoint(\n",
    "    config[\"decoder_model_path\"],\n",
    "    in_ch=config[\"in_ch\"],\n",
    "    out_ch=config[\"out_ch\"],\n",
    "    diffusion_model=diffusion,\n",
    "    latent_dim=config[\"latent_dim\"],\n",
    ")\n",
    "\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diffusion.eval()\n",
    "decoder.eval()\n",
    "\n",
    "gen_data = []\n",
    "music_data = []\n",
    "mixture_data = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_set):\n",
    "        melody, mixture, music, track = batch\n",
    "        shape = (music.shape[0], 5, 64, 72)\n",
    "        \n",
    "        mixture = mixture.to(device)\n",
    "        data = diffusion(mixture, shape, device=device, eta=0, mode=\"ddim\")\n",
    "        data = decoder(data)\n",
    "        \n",
    "        data = nn.Sigmoid()(data)\n",
    "        data = (data >= 0.5).to(torch.float32)\n",
    "        data = data * mixture\n",
    "        \n",
    "        data = data.detach().cpu().numpy()\n",
    "        data = np.transpose(data, [0, 2, 3, 1])\n",
    "        \n",
    "        music = music.detach().cpu().numpy()\n",
    "        music = np.transpose(music, [0, 2, 3, 1])\n",
    "        \n",
    "        mixture = mixture.detach().cpu().numpy()\n",
    "        mixture = np.transpose(mixture, [0, 2, 3, 1])\n",
    "        \n",
    "        gen_data.append(data)\n",
    "        music_data.append(music)\n",
    "        mixture_data.append(mixture)\n",
    "        \n",
    "gen_data = np.vstack(gen_data)\n",
    "music_data = np.vstack(music_data)\n",
    "mixture_data = np.vstack(mixture_data)\n",
    "\n",
    "print(\"\\ngen_data shape :\", gen_data.shape)\n",
    "print(\"music_data shape :\", music_data.shape)\n",
    "print(\"mixture_data shape :\", mixture_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistency loss\n",
    "consistency = consistency_loss(mixture_data, gen_data)\n",
    "print(\"consistency : %f\" % (consistency))\n",
    "\n",
    "# diversity loss\n",
    "diversity = diversity_loss(mixture_data, music_data, gen_data)\n",
    "print(\"diversity : %f\" % (diversity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_time = 0.18\n",
    "file_path = \"./samples/\"\n",
    "\n",
    "for idx in range(5):\n",
    "    pm1 = play_pianoroll(gen_data[idx], event_time=event_time)\n",
    "    pm2 = play_pianoroll(music_data[idx], event_time=event_time)\n",
    "    \n",
    "    wav1 = pm1.fluidsynth(fs=16000).astype(np.float32)\n",
    "    wav2 = pm2.fluidsynth(fs=16000).astype(np.float32)\n",
    "    \n",
    "    file_name_1 = str(idx+1) + \"_music_from_mixture.wav\"\n",
    "    file_name_2 = str(idx+1) + \"_original_music.wav\"\n",
    "    \n",
    "    # # save midi file as wav\n",
    "    # write(file_path + file_name_1, 16000, wav1.astype(np.float32))\n",
    "    # write(file_path + file_name_2, 16000, wav2.astype(np.float32))\n",
    "\n",
    "    # visualize pianoroll\n",
    "    file_name = str(idx+1) + \".png\"\n",
    "    plot_two_pianoroll(gen_data[idx], music_data[idx], save_path=file_path+file_name,\n",
    "                       SIZE=[10, 10], CHAR_FONT_SIZE=15, NUM_FONT_SIZE=13, LABEL_PAD=8)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
